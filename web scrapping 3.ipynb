{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File = open(\"out.csv\", \"a\")\n",
    "  \n",
    "HEADERS = ({'User-Agent':\n",
    "           'Mozilla/5.0 (X11; Linux x86_64) \n",
    "                AppleWebKit/537.36 (KHTML, like Gecko) \n",
    "                    Chrome/44.0.2403.157 Safari/537.36',\n",
    "                           'Accept-Language': 'en-US, en;q=0.5'})\n",
    "  \n",
    "webpage = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        title = soup.find(\"span\", \n",
    "                          attrs={\"id\": 'productTitle'})\n",
    "        title_value = title.string\n",
    "  \n",
    "        title_string = title_value\n",
    "                    .strip().replace(',', '')\n",
    "            \n",
    "except AttributeError:\n",
    "  \n",
    "        title_string = \"NA\"\n",
    "  \n",
    "        print(\"product Title = \", title_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    " \n",
    "    file = open(\"url.txt\", \"r\")\n",
    "  \n",
    "   \n",
    "    for links in file.readlines():\n",
    "        main(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(URL):\n",
    "   \n",
    "    File = open(\"out.csv\", \"a\")\n",
    "  \n",
    "   \n",
    "    HEADERS = ({'User-Agent':\n",
    "                'Mozilla/5.0 (X11; Linux x86_64) \n",
    "                    AppleWebKit/537.36 (KHTML, like Gecko) \n",
    "                            Chrome/44.0.2403.157 Safari/537.36',\n",
    "                                'Accept-Language': 'en-US, en;q=0.5'})\n",
    "  \n",
    "    \n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "  \n",
    "   \n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "  \n",
    "   \n",
    "    try:\n",
    "      \n",
    "        title = soup.find(\"span\", \n",
    "                          attrs={\"id\": 'productTitle'})\n",
    "  \n",
    "       \n",
    "        title_value = title.string\n",
    "  \n",
    "       \n",
    "        title_string = title_value.strip().replace(',', '')\n",
    "  \n",
    "    except AttributeError:\n",
    "        title_string = \"NA\"\n",
    "    print(\"product Title = \", title_string)\n",
    "  \n",
    "   \n",
    "    File.write(f\"{title_string},\")\n",
    "  \n",
    "   \n",
    "    try:\n",
    "        price = soup.find(\n",
    "            \"span\", attrs={'id': 'priceblock_ourprice'})\n",
    "                                .string.strip().replace(',', '')\n",
    "       \n",
    "    except AttributeError:\n",
    "        price = \"NA\"\n",
    "    print(\"Products price = \", price)\n",
    "  \n",
    "   \n",
    "    File.write(f\"{price},\")\n",
    "  \n",
    "   \n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={\n",
    "                           'class': 'a-icon a-icon-star a-star-4-5'})\n",
    "                                    .string.strip().replace(',', '')\n",
    "  \n",
    "    except AttributeError:\n",
    "  \n",
    "        try:\n",
    "            rating = soup.find(\n",
    "                \"span\", attrs={'class': 'a-icon-alt'})\n",
    "                                .string.strip().replace(',', '')\n",
    "        except:\n",
    "            rating = \"NA\"\n",
    "    print(\"Overall rating = \", rating)\n",
    "  \n",
    "    File.write(f\"{rating},\")\n",
    "  \n",
    "    try:\n",
    "        review_count = soup.find(\n",
    "            \"span\", attrs={'id': 'acrCustomerReviewText'})\n",
    "                                .string.strip().replace(',', '')\n",
    "  \n",
    "    except AttributeError:\n",
    "        review_count = \"NA\"\n",
    "    print(\"Total reviews = \", review_count)\n",
    "    File.write(f\"{review_count},\")\n",
    "  \n",
    "   \n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id': 'availability'})\n",
    "        available = available.find(\"span\")\n",
    "                    .string.strip().replace(',', '')\n",
    "  \n",
    "    except AttributeError:\n",
    "        available = \"NA\"\n",
    "    print(\"Availability = \", available)\n",
    "  \n",
    "    \n",
    "    File.write(f\"{available},\\n\")\n",
    "  \n",
    "   \n",
    "    File.close()\n",
    "  \n",
    "  \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    file = open(\"url.txt\", \"r\")\n",
    "  \n",
    "    \n",
    "    for links in file.readlines():\n",
    "        main(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url=â€œhttps://www.google.com/search?q={q}&tbm=isch&tbs=sur%3Afc&hl=en&ved=0CAIQpwVqFwoTCKCa1c6s4-oCFQAAAAAdAAAAABAC&biw=1251&bih=568\" \n",
    "\n",
    "driver.get(search_url.format(q='Car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgResults = driver.find_elements_by_xpath(\"//img[contains(@class,'Q4LuWd')]\")\n",
    "totalResults=len(imgResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_urls = set()\n",
    "for i in  range(0,len(imgResults)):\n",
    "    img=imgResults[i]\n",
    "    try:\n",
    "        img.click()\n",
    "        time.sleep(2)\n",
    "        actual_images = driver.find_elements_by_css_selector('img.n3VNCb')\n",
    "        for actual_image in actual_images:\n",
    "            if actual_image.get_attribute('src') and 'https' in actual_image.get_attribute('src'):\n",
    "                img_urls.add(actual_image.get_attribute('src'))\n",
    "    except ElementClickInterceptedException or ElementNotInteractableException as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Qurantine/Blog/WebScrapping/Dataset1')\n",
    "baseDir=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url in enumerate(img_urls):\n",
    "    file_name = f\"{i:150}.jpg\"    \n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "except Exception as e:\n",
    "        print(f\"ERROR - COULD NOT DOWNLOAD {url} - {e}\")\n",
    "\n",
    "try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        \n",
    "        file_path = os.path.join(baseDir, file_name)\n",
    "        \n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        print(f\"SAVED - {url} - AT: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - COULD NOT SAVE {url} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wb_name = \"xxx.xlsx\" \n",
    "\n",
    "\n",
    "wb = load_workbook(wb_name, data_only = True)\n",
    "ws = wb['sheet_name']\n",
    "address_list =[]\n",
    "link_col = xx  \n",
    "\n",
    "coord_prospects = pd.DataFrame() \n",
    "for t,a in enumerate(address_list):\n",
    "    print (\"Geocoding...\",t+1,\"/\",len(address_list),str(round(t/len(address_list)*100,2)),\"%\",\" : \", a)\n",
    "    add.clear()\n",
    "    add.send_keys(a)\n",
    "    try:\n",
    "        search1 = driver.find_element_by_xpath('//*[@id=\"search-form\"]/div[1]/span[2]').click()\n",
    "        time.sleep(3)\n",
    "        search2 = driver.find_element_by_xpath('//*[@id=\"search-form\"]/div[1]/span[2]').click()\n",
    "        time.sleep(3)\n",
    "    except ElementClickInterceptedException:\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   search = driver.find_element_by_xpath('//*[@id=\"search-form\"]/div[1]/span[2]').click()\n",
    "    lat=driver.find_element_by_id('display_lat')\n",
    "    lng=driver.find_element_by_id('display_lng')\n",
    "    street=driver.find_element_by_id('display_address')\n",
    "    city=driver.find_element_by_id('display_city')\n",
    "    postcode=driver.find_element_by_id('display_zip')\n",
    "    state=driver.find_element_by_id('display_state')\n",
    "    county=driver.find_element_by_id('display_county')\n",
    "    country=driver.find_element_by_id('display_country')\n",
    "    latlng = pd.DataFrame({'Latitude':pd.Series(lat.text),\n",
    "                            'Longitude':pd.Series(lng.text),\n",
    "                            'Street':pd.Series(street.text),\n",
    "                            'City':pd.Series(city.text),\n",
    "                            'Postcode':pd.Series(postcode.text),\n",
    "                            'State':pd.Series(state.text),\n",
    "                            'County':pd.Series(county.text),\n",
    "                            'Country':pd.Series(country.text)})\n",
    "    coord_prospects = coord_prospects.append(latlng, ignore_index=True)\n",
    "    print(coord_prospects.tail(1))\n",
    "    print(\"   \")\n",
    "\n",
    "coord_prospects.to_excel('xxxx.xlsx') \n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"xxx.xlsx\")\n",
    "\n",
    "geocoder = googlemaps.Client(key=api)\n",
    "df['Latitude'] = None\n",
    "df['Longitude'] = None\n",
    "df['Google Address'] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "    print(\"Geocoding...\"+\" \"+str(i)+\"/\"+str(len(df)) + \" \" + str(round(i/len(df)*100,2))+\"%\")\n",
    "    result = geocoder.geocode(df.loc[i,\"xxx\"]) \n",
    "    try:\n",
    "        lat = result[0][\"geometry\"][\"location\"][\"lat\"]\n",
    "        lng = result[0][\"geometry\"][\"location\"][\"lng\"]\n",
    "        address = result[0][\"formatted_address\"]\n",
    "        df.loc[i,\"Latitude\"] = lat\n",
    "        df.loc[i,\"Longitude\"] = lng\n",
    "        df.loc[i,\"Google Address\"] = address\n",
    "    except:\n",
    "        lat=None\n",
    "        lng=None\n",
    "        location=None\n",
    "print(df)\n",
    "df.to_excel(\"xxx.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= â€˜MSFT \n",
    "url_is = â€˜https://finance.yahoo.com/quote/' + index + â€˜/financials?p=â€™ + index\n",
    "url_bs = â€˜https://finance.yahoo.com/quote/' + index +â€™/balance-sheet?p=â€™ + index\n",
    "url_cf = â€˜https://finance.yahoo.com/quote/' + index + â€˜/cash-flow?p=â€™+ index\n",
    "\n",
    "read_data = ur.urlopen(url_is).read() \n",
    "soup_is= BeautifulSoup(read_data,â€™lxmlâ€™)\n",
    "\n",
    "ls= [] \n",
    "for l in soup_is.find_all(â€˜divâ€™): \n",
    "  \n",
    "  ls.append(l.string) \n",
    "ls = [e for e in ls if e not in (â€˜Operating Expensesâ€™,â€™Non-recurring Eventsâ€™)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\patel\\OneDrive\\Desktop\\chromedriver.exe')\n",
    "driver.get(\"https://www.forbes.com/billionaires/list/50/#version:realtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"rank\"]')))\n",
    "Name =list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"name\"]')))\n",
    "Net_Worth = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"networth\"]')))\n",
    "wait_button = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[6]')))\n",
    "Source = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[7]')))\n",
    "Country = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[8]')))\n",
    "wait_button = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank = [(re.sub('[\\n , . - #]+',\"\", i)) for i in Rank]\n",
    "Name = [(re.sub('[\\n,.-]+',\"\", i)) for i in Name]\n",
    "Net_Worth = [(re.sub('[\\n , - # B $]+',\"\", i)) for i in Net_Worth]\n",
    "Age = [(re.sub('[\\n,.-]', '', i)) for i in Age]\n",
    "Source = [(re.sub('[\\n,.-]', '', i)) for i in Source]\n",
    "Country = [(re.sub('[\\n,.-]', '', i)) for i in Country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = zip(Rank, Name, Net_Worth, Age, Source, Country)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
